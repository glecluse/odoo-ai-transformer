# pages/2_üìñ_Guide_d_Utilisation.py

import streamlit as st

st.set_page_config(layout="wide", page_title="Guide d'Utilisation - Odoo AI Transformer")

# --- Gardien d'Authentification ---
if not st.session_state.get('is_logged_in', False):
    st.error("üîí Acc√®s r√©serv√© aux utilisateurs connect√©s.")
    st.info("Veuillez vous connecter depuis la page d'accueil pour acc√©der √† ce guide.")
    st.page_link("app.py", label="Retour √† l'accueil", icon="üè†")
    st.stop()
# --- Fin du Gardien ---


st.title("üìñ Guide Complet : de Z√©ro √† votre Dashboard Automatis√©")
st.markdown("""
Ce guide vous accompagne √† chaque √©tape du processus, depuis la configuration initiale de votre environnement Google Cloud jusqu'√† l'automatisation compl√®te de votre pipeline de donn√©es Odoo, **le tout depuis l'interface web de la console GCP**.
""")

st.divider()

# ==============================================================================
# PHASE 1 : PR√âREQUIS GCP
# ==============================================================================
st.header("Phase 1 : Pr√©requis - Votre Compte Google Cloud (GCP)")
st.warning("Cette phase est √† r√©aliser **une seule fois**. Si votre projet GCP est d√©j√† pr√™t, vous pouvez passer √† la phase 2.", icon="‚òùÔ∏è")

with st.expander("Cliquez ici pour le d√©tail de la configuration initiale de GCP"):
    
    st.subheader("A. Cr√©er ou acc√©der √† votre compte GCP")
    st.markdown("""
    1.  **Compte et Projet :** Assurez-vous d'avoir un [compte Google Cloud](https://console.cloud.google.com/) avec un projet cr√©√© et un compte de facturation associ√©.
    2.  **Activer les API :** Dans la console, cherchez et activez les services suivants : `Cloud Functions`, `Cloud Build`, `Secret Manager`, `BigQuery API`, `Cloud Scheduler API`, et `BigQuery Data Transfer Service`.
    """)

# ==============================================================================
# PHASE 2 : UTILISATION DE L'APPLICATION
# ==============================================================================
st.header("Phase 2 : Utilisation de l'Application")
st.markdown("""
Sur la page **Application**, suivez le flux complet :
1.  **Connectez-vous √† Odoo**.
2.  **D√©crivez votre besoin** et g√©n√©rez le plan de transformation.
3.  **Ex√©cutez le plan**. Cela va extraire les donn√©es d'Odoo et les d√©poser dans un fichier sur votre **Google Cloud Storage**.
4.  **G√©n√©rez les artefacts GCP**. L'application vous fournira tout le code et les commandes n√©cessaires. Gardez cet onglet ouvert.
""")

st.divider()

# ==============================================================================
# PHASE 3 : D√âPLOIEMENT SUR GCP
# ==============================================================================
st.header("Phase 3 : D√©ploiement sur votre projet GCP")
st.success("Vous allez maintenant configurer votre infrastructure sur GCP en utilisant les artefacts g√©n√©r√©s.")

st.subheader("√âtape 5 : Cr√©er le Secret pour le mot de passe Odoo")
st.markdown("""
1.  **Ouvrir le Cloud Shell :** En haut √† droite de la console Google Cloud, cliquez sur l'ic√¥ne **[>_]** ("Activer Cloud Shell").
2.  **Copier et coller la commande :** Copiez la commande `gcloud secrets create ...` fournie par l'application et collez-la dans le Cloud Shell. Appuyez sur **Entr√©e** et autorisez si n√©cessaire. Votre cl√© API est maintenant stock√©e en toute s√©curit√©.
""")

st.subheader("√âtape 6 : D√©ployer la Cloud Function")
st.markdown("""
Suivez les √©tapes pour cr√©er une **Cloud Function** depuis la console, en utilisant le code `main.py` et `requirements.txt` fournis par l'application. N'oubliez pas l'√©tape cruciale : donner au **compte de service** de la fonction le r√¥le **`Accesseur de secrets de Secret Manager`** sur le secret que vous venez de cr√©er.
""")

st.subheader("√âtape 7 : Cr√©ation de la Table et de la Vue dans BigQuery")
st.markdown("""
**A. Cr√©er une table native depuis Cloud Storage (chargement initial)**
1.  Allez dans la console **[BigQuery](https://console.cloud.google.com/bigquery)**.
2.  Dans le panneau "Explorateur", trouvez votre projet, cliquez sur les trois points √† c√¥t√© de votre ensemble de donn√©es (dataset) et choisissez **"Cr√©er une table"**.
3.  Remplissez le formulaire de cr√©ation :
    - **Cr√©er une table √† partir de :** `Google Cloud Storage`.
    - **S√©lectionner le fichier depuis le bucket GCS :** Cliquez sur "Parcourir". Naviguez et s√©lectionnez le fichier `.parquet` g√©n√©r√© par la fonction.
    - **Format du fichier :** `Parquet`.
    - **Table :** Donnez un nom √† votre table de donn√©es brutes (ex: `odoo_ventes_raw`).
    - **Sch√©ma :** Cochez la case **"D√©tection automatique"**.
4.  Cliquez sur **"Cr√©er une table"**.

**B. Cr√©er la vue par-dessus la table**
1.  Maintenant, copiez le code SQL pour la **"Vue BigQuery"** g√©n√©r√© par notre application.
2.  Collez cette requ√™te dans l'√©diteur de requ√™tes de BigQuery et cliquez sur **"Ex√©cuter"**.
""")


# ==============================================================================
# PHASE 4 : AUTOMATISATION
# ==============================================================================
st.header("Phase 4 : Automatisation Compl√®te du Pipeline")

st.subheader("A. Automatiser l'ex√©cution de la Cloud Function (Cloud Scheduler)")
st.markdown("""
Cette t√¢che va extraire les donn√©es d'Odoo chaque jour.
1.  Allez dans **[Cloud Scheduler](https://console.cloud.google.com/cloudscheduler)** et cr√©ez une t√¢che.
2.  **Fr√©quence :** `0 5 * * *` (tous les jours √† 5h du matin).
3.  **Cible :** `HTTP`, avec l'URL de votre Cloud Function.
4.  **Authentification :** `Ajouter un jeton OIDC`, avec le compte de service de votre fonction.
""")

st.subheader("B. Automatiser le transfert de Cloud Storage vers BigQuery")
st.markdown("""
Cette seconde t√¢che va automatiquement mettre √† jour votre table BigQuery avec le nouveau fichier d√©pos√© par la Cloud Function.
1.  Allez dans la console **[BigQuery Data Transfer Service](https://console.cloud.google.com/bigquery/transfers)** et cliquez sur **"CR√âER UN TRANSFERT"**.
2.  **Source :** `Google Cloud Storage`.
3.  **Planification :** Choisissez une heure **post√©rieure** √† celle de la Cloud Function (ex: "Tous les jours √† 6h00").
4.  **Param√®tres de destination :**
    - **Table :** Entrez le nom de la table native que vous avez cr√©√©e (ex: `odoo_ventes_raw`).
    - **Pr√©f√©rence d'√©criture :** **`Overwrite`** (√âcraser).
5.  **D√©tails de la source de donn√©es :**
    - **Chemin Cloud Storage :** Indiquez le chemin vers votre fichier, avec un joker (ex: `gs://VOTRE_BUCKET/rapport_ventes_mensuel.parquet`).
6.  Cliquez sur **"Enregistrer"**.
""")

st.divider()

# ==============================================================================
# PHASE 5 : VISUALISATION
# ==============================================================================
st.header("Phase 5 : Connexion de votre Outil de Business Intelligence")
st.markdown("F√©licitations ! üöÄ Votre pipeline est 100% fonctionnel et automatis√©. La derni√®re √©tape est la plus gratifiante : connecter votre outil de BI √† la **vue BigQuery** que vous avez cr√©√©e.")
st.info("Choisissez votre outil dans la liste ci-dessous pour obtenir un guide de connexion d√©taill√©.")

tool_options = [
    "S√©lectionnez un outil...",
    "Looker Studio",
    "Microsoft Power BI",
    "Tableau",
    "Metabase",
    "Qlik Sense",
    "Google Sheets",
    "Autre outil (g√©n√©rique)"
]
selected_tool = st.selectbox("Choisissez votre outil de visualisation :", tool_options)

if selected_tool == "Looker Studio":
    st.success("""
    **Mode Op√©ratoire pour Looker Studio :**
    1.  Ouvrez [Looker Studio](https://lookerstudio.google.com/).
    2.  Cliquez sur **Cr√©er > Source de donn√©es**.
    3.  S√©lectionnez le connecteur **BigQuery**.
    4.  Naviguez jusqu'√† votre **Projet GCP > Dataset > Vue** pour la s√©lectionner.
    5.  Cliquez sur **Associer**.
    """)

elif selected_tool == "Microsoft Power BI":
    st.success("""
    **Mode Op√©ratoire pour Microsoft Power BI :**
    1.  Dans Power BI Desktop, cliquez sur **Obtenir des donn√©es > Google BigQuery**.
    2.  Authentifiez-vous avec votre compte Google.
    3.  Dans le "Navigateur", trouvez votre **Projet > Dataset > Vue** et cochez-la.
    4.  Cliquez sur **Charger**.
    """)

elif selected_tool == "Tableau":
    st.success("""
    **Mode Op√©ratoire pour Tableau :**
    1.  Dans Tableau, sous "Se connecter", choisissez **Google BigQuery**.
    2.  Authentifiez-vous avec votre compte Google.
    3.  S√©lectionnez votre **Projet de facturation > Projet > Dataset** et faites glisser votre **Vue** dans l'espace de travail.
    """)

elif selected_tool == "Metabase":
    st.success("""
    **Mode Op√©ratoire pour Metabase :**
    *Pr√©requis : Avoir un fichier JSON de cl√© de compte de service GCP.*
    1.  Dans **Admin > Bases de donn√©es > Ajouter une base de donn√©es**.
    2.  Choisissez **Google BigQuery**.
    3.  Importez votre fichier JSON de cl√© et sp√©cifiez les datasets √† scanner.
    4.  Cliquez sur **Enregistrer**.
    """)

elif selected_tool == "Qlik Sense":
    st.success("""
    **Mode Op√©ratoire pour Qlik Sense :**
    1.  Dans le **Gestionnaire de donn√©es**, cliquez sur **Ajouter des donn√©es**.
    2.  S√©lectionnez le connecteur **Google BigQuery**.
    3.  Apr√®s vous √™tre authentifi√©, s√©lectionnez votre projet, dataset et la vue √† charger.
    """)

elif selected_tool == "Google Sheets":
    st.success("""
    **Mode Op√©ratoire pour Google Sheets :**
    1.  Dans un nouveau Google Sheet, allez dans **Donn√©es > Connecteurs de donn√©es > Se connecter √† BigQuery**.
    2.  S√©lectionnez votre projet et trouvez votre vue.
    3.  Importez les donn√©es.
    """)

elif selected_tool == "Autre outil (g√©n√©rique)":
    st.warning("""
    **Principe G√©n√©ral de Connexion :**
    1.  Cherchez le connecteur **"Google BigQuery"** dans votre outil.
    2.  Authentifiez-vous.
    3.  Naviguez ou sp√©cifiez le chemin vers votre vue : `ID_Projet.ID_Dataset.Nom_Vue`.
    """)